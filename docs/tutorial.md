# Tutorial

This tutorial assumes a GNU/Linux system and that tatajuba is already installed. For installation instructions please
refer to the [README file](../README.md) in the entry page of the github repository. 

We also assume that some tools are installed or that you can handle installing missing ones.

## Downloading the data

If you download the [example_tutorial.txz](example_tutorial.txz) file and expand it with
```bash
tar Jxvf example_tutorial.txz
```
It will generate a structure similar to below:
```console
example_tutorial
├── data
│   ├── borde
│   │   ├── genes.gff
│   │   ├── sequences.fa
│   │   └── snpEffectPredictor.bin
│   └── campy
│       ├── genes.gff
│       ├── sequences.fa
│       └── snpEffectPredictor.bin
├── GCF_000148705.1_ASM14870v1_genomic.fna -> data/campy/sequences.fa
├── GCF_000148705.1_ASM14870v1_genomic.fna.amb
├── GCF_000148705.1_ASM14870v1_genomic.fna.ann
├── GCF_000148705.1_ASM14870v1_genomic.fna.bwt
├── GCF_000148705.1_ASM14870v1_genomic.fna.pac
├── GCF_000148705.1_ASM14870v1_genomic.fna.sa
├── GCF_000148705.1_ASM14870v1_genomic.gff -> data/campy/genes.gff
├── outdir
│   ├── 1.vcf.gz
│   ├── 2.vcf.gz
│   ├── 4.vcf.gz
│   ├── 5.vcf.gz
│   ├── 7.vcf.gz
│   ├── per_sample_average_length.tsv
│   ├── per_sample_modal_frequency.tsv
│   ├── per_sample_proportional_coverage.tsv
│   ├── selected_tracts_annotated.tsv
│   ├── selected_tracts_unknown.tsv
│   ├── tract_list.tsv
│   └── variable_tracts.bed
├── reads
│   ├── ERR1701019.fastq
│   ├── ERR1701029.fastq
│   ├── ERR1701049.fastq
│   ├── ERR1701059.fastq
│   └── ERR1701079.fastq
├── snpEff.config
└── tutorial.md
```

Some files may be missing, as those generated by BWA (`GCF_000148705.1_ASM14870v1_genomic.fna.*`) and tatajuba output,
but they can be reconstructed.

The `data` directory and the `snpEff.config` file are for _snpEff_. The `reads` folder contains (severely subsampled)
read files to be used in this tutorial. The `GCF_000148705.1_ASM14870v1_genomic.*` files are the GFF and FASTA files
used as the reference genome.

This data set is a small subset of the [_Campylobacter_ data used in the manuscript](211108.figures_snippy_comparison.ipynb).

## Running tatajuba

#### The reference genome files
Tatajuba implements the [BWA](https://github.com/lh3/bwa) library for reference mapping, which relies on index
files it generates from the FASTA file *if they are missing*.
That is, tatajuba does not overwrite the `.amb`, `.ann`, `.bwt` etc. files every time it runs, therefore if you modify
the fasta file please delete these index files so that tatajuba can reconstruct them.

By the way, these index files are generated in the same directory as the fasta file, thus please copy the reference
files to a directory where you have write permissions. 
The FASTA file is not needed if and only if the sequences are embedded within the GFF file, as is the case for
[prokka](https://github.com/tseemann/prokka)'s output.
Tatajuba will then generate a fasta file from it (together with its index files).

For this tutorial, I have downloaded the references from the [Assembly](https://www.ncbi.nlm.nih.gov/assembly/GCF_000493495.1) 
database on NCBI, since I need both the genomic GFF and the FASTA files with corresponding names.
The [RefSeq](https://www.ncbi.nlm.nih.gov/nuccore/NC_022529.1) database also provides this, as well as obviously prokka
output.
The GFF format talks about contigs or chromosomes, which are the genomic FASTA sequences (genome or plasmid). Thus
tatajuba (and its documentation) use these words interchangeably. 

#### The command line
As described in the [installation instructions](../README.md) you may have installed tatajuba with conda or from the
source code. You may even have both, in which case you need to find which one you are using:

```console
ubuntu@local$ locate bin/tatajuba # show possible locations of the executable
/home/ubuntu/local/bin/tatajuba
/home/ubuntu/miniconda3/bin/tatajuba
ubuntu@local$  # show which executable is default (i.e. which one is called if you just type "tatajuba")
ubuntu@local$ which tatajuba 
/home/ubuntu/miniconda3/bin/tatajuba
```
If you want to choose explicitly the executable, you only need to write its full path:

```console
ubuntu@local$ /home/ubuntu/bin/tatajuba
Error when reading arguments from command line:
tatajuba: missing option -g|--gff=<genome.gff3|genome.gff3.gz>
tatajuba: missing option <fastq files>

tatajuba 1.0.4
Compare histograms of homopolymeric tract lengths, within context.
The complete syntax is:

 tatajuba  [-h|--help] [-v|--version] [-p|--paired] [-b|--keep_bias] [-V|--vcf] [-k|--kmer={2,...,32}] [-m|--minsize={1,...,32}] [-i|--minreads=<int>] [-d|--maxdist=<int>] [-l|--leven=<int>] [-t|--nthreads=<int>] -g|--gff=<genome.gff3|genome.gff3.gz> [-f|--fasta=<genome.fna|genome.fna.gz>] <fastq files> [<fastq files>]... [-o|--outdir=<file>]

  -h, --help                       print a longer help and exit
  -v, --version                    print version and exit
  -p, --paired                     paired end (pairs of) files
  -b, --keep_bias                  keep biased tracts, i.e. present only in reverse or only in forward strains (default=remove)
  -V, --vcf                        generate VCF files for each sample, around the HT regions (EXPERIMENTAL) (default=not to save)
  -k, --kmer={2,...,32}            kmer size flanking each side of homopolymer (default=25)
  -m, --minsize={1,...,32}         minimum homopolymer tract length to be compared (default=4)
  -i, --minreads=<int>             minimum number of reads for tract+context to be considered (default=5)
  -d, --maxdist=<int>              maximum distance between kmers of a flanking region to merge them into one context (default=1)
  -l, --leven=<int>                levenshtein distance between flanking regions to merge them into one context (after ref genome mapping)
  -t, --nthreads=<int>             suggested number of threads (default is to let system decide; I may not honour your suggestion btw)
  -g, --gff=<genome.gff3|genome.gff3.gz> reference genome file in GFF3, preferencially with sequence
  -f, --fasta=<genome.fna|genome.fna.gz> reference genome file in fasta format, if absent from GFF3
  <fastq files>                    fastq file with reads (weirdly, fasta also possible as long as contains all reads and not only contigs)
  -o, --outdir=<file>              output directory, or 'random' for generating random dir name (default=current dir '.')
```

In the example above it is complaining that you need at least to provide a GFF3 file and the fastq files with the
samples' reads.
So if we provide the GFF3 file and the reads, it will complain that the sequences are missing from this particular GFF3 file:
```console
ubuntu@local$ /home/ubuntu/bin/tatajuba -g GCF_000148705.1_ASM14870v1_genomic.gff reads/ERR17010*
[ error ] No fasta provided and GFF3 file doesn't contain sequences
 You must provide a fasta file with reference genome sequence(s) that match the GFF3 features, or you should find a GFF3 file with a '##FASTA' section at the end.

[note to developers] If you want to debug me, set a breakpoint on function biomcmc_error()
```
Which we already knew is quite common. Luckily we also have the fasta file (`GCF_000148705.1_ASM14870v1_genomic.fna`). 
The reads used in this tutorial are single reads (and thus the `-p` option is not needed), and since they are a small
subsample of the original files we will decresase the minimum HT coverage to 3 reads (`-i 3`).
We also define the output directory to be `outdir` and we ask tatajuba to generate VCF files (`-V`).
The full command would be:

```console
ubuntu@local$ /home/ubuntu/bin/tatajuba -g GCF_000148705.1_ASM14870v1_genomic.gff -V \
              -f GCF_000148705.1_ASM14870v1_genomic.fna -o outdir -i 3  reads/ERR17010*
[warning] File 'outdir/' already exists; I'll assume it's a directory. Contents will be overwritten
[warning] Decreasing number of threads to match number of samples
tatajuba 1.0.4
Reference genome fasta file: GCF_000148705.1_ASM14870v1_genomic.fna
Reference GFF3 file prefix:  GCF_000148705.1_ASM14870v1_genomic
Output directory:            outdir/
Number of samples:               5 (single-end)
Max distance per flanking k-mer:       1
Levenshtein distance for merging:      2
Flanking k-mer size (context):        25
Min tract length to consider:          4
Min depth of tract lengths:            3
Remove biased tracts:             yes
Number of threads (requested or optimised):   5
Assuming single-end samples
Read GFF3 reference genome in        0.088360 secs

processing file reads/ERR1701019.fastq
processing file reads/ERR1701059.fastq
processing file reads/ERR1701079.fastq
processing file reads/ERR1701049.fastq
processing file reads/ERR1701029.fastq
[bwa_aln_from_vector] 24.46399 sec
reads/ERR1701029.fastq:  21402 found and   7571 context+tracts were not found in reference
[bwa_aln_from_vector] 42.52276 sec
reads/ERR1701019.fastq:  47628 found and   3250 context+tracts were not found in reference
[bwa_aln_from_vector] 48.69241 sec
[bwa_aln_from_vector] 48.57682 sec
reads/ERR1701079.fastq:  51177 found and   2558 context+tracts were not found in reference
reads/ERR1701059.fastq:  54123 found and   1953 context+tracts were not found in reference
[bwa_aln_from_vector] 54.10467 sec
reads/ERR1701049.fastq:  52852 found and   2257 context+tracts were not found in reference
Modifying sample names: Removing prefix 'reads/ERR17010' from sample names.
Modifying sample names: Removing suffix '9.fastq' from sample names.
From 63473 tracts, 46969 interesting ones are annotated and 4058 interesting ones are not annotated
Will save VCF files with gzipped compression
Internal (threaded) timer::        2.054656 secs to read and generate initial histograms
Internal (threaded) timer::       59.129513 secs to merge and map histograms
Internal (threaded) timer::        0.079142 secs to compare across sample genomes
Internal (threaded) timer::        0.569195 secs to compare with reference
Non-threaded timing      ::       16.337782 secs
```

#### interpreting the screen output

If you have paired reads, the output lines would look something like

```console
processing paired files ERR1701022/ERR1701022_1.fastq.gz and ERR1701022/ERR1701022_2.fastq.gz
```
And you can check them to see if tatajuba is using them in the desired order. 



## The output

# Downstream analyses 

## Annotating the VCF files with variant effect information

As we mention in the [README file](../README.md), there are a few tools available for exploring the functional effect of
mutations: 
[bcftools consequence calling](https://samtools.github.io/bcftools/howtos/csq-calling.html), 
[Ensembl Variant Effect Predictor (VEP)](https://www.ensembl.org/info/docs/tools/vep/index.html), and 
[snpEff](http://pcingola.github.io/SnpEff/).
They rely on the VCF file for the samples, together with the GFF3 and FASTA files for the reference genome.
We recommend `snpEff` or `VEP`, since `bcftools csq`, supports only ENSEMBL GFF3 files.

Ideally the VCF files should be generated from the BAM/SAM files, i.e. using reference-based assembly programs 
like [minimap2](https://github.com/lh3/minimap2) or [bwa](https://github.com/lh3/bwa).
[Snippy](https://github.com/tseemann/snippy) generates not only the BAM alignment and its corresponding `snps.filt.vcf` files, 
as it annotates the predicted effects with `snpEff` into the `snps.vcf` files. 
Howver keep in mind that the "core SNPs" and [further analyses from snippy-multi](https://github.com/tseemann/snippy#core-snp-phylogeny) 
by design will exclude most information from the HTs (which are indels). 

Tatajuba generates a BED file which can be used to filter the regions harbouring homopolymeric tracts. 
But it also experimentally outputs individual VCF files with minimal information (i.e. no quality information etc.).
These VCF files work well with `snpEff` and `bcftools` but remreber that the gzip format output by tatajuba is **not**
compatible with `bcftools`. 
Conversion is easy though, and if in doubt simply unconpress everything with `gunzip *.gz`. The VCF files are not big.

In this example we will describe how to use `snpEff`. 

### creating a database
For instructions on how to use your own GFF3 file, which is almost always the case, see [this discussion](https://www.biostars.org/p/50963/) and 
also [the official documentation](https://pcingola.github.io/SnpEff/se_buildingreg/#option-1-using-a-gff-file).
In a nutshell, you'll need a configuration file, which we call `snpEff.config`, with locations of the databases.
In our case the relevant lines look like

```
data.dir = ./data/
campy.genome: campy
borde.genome: borde
```

This means that there are two genome databases called "campy" and "borde", which are in fact subdirectories below
`./data/`.
As usual, we learned this [by being one with the Torstenverse](https://github.com/tseemann/snippy). 


Within each of these directories, you'll need one GFF file, named `genes.gff`, with its corresponding FASTA file,
called `sequences.fa`.
If you have been playing with tatajuba you should already have these files :wink:.
Then you tell `snpEff` to generate the database from these files:

```
zcat ../myfiles/GCF_000148705.1_ASM14870v1_genomic.gff.gz > data/campy/genes.gff
zcat ../myfiles/GCF_000148705.1_ASM14870v1_genomic.fna.gz > data/campy/sequences.fa
snpEff build -c snpEff.config -gff3 campy
```

In the example above I copy them from two zipped files from another directory, but if you downloaded the 
[example data set](example_tutorial.txz) then the files should already be there, and with links in the current directory.

### running snpEff

To actually run `snpEff` you need to tell it where the configuration file is, and which database you want to use (in our
case, the freshly created "campy"):

```bash
snpEff ann -c snpEff.config campy concatenated.vcf > concatenated.annotated.vcf
```

In this example we are annotating all variants, across all samples since we are using the file `hand_merged.vcf` [from
above](#merging-all-variants-from-the-vcf-file).


## Concatenating all variants from the VCF files

In this example we assume you want to have a summary of all variants across all samples (to get all possible mutational
effects, for instance). 
One alternative is to [merge all samples into a multi-sample VCF file](#merging-samples-with-bcftools). 
However a simpler alternative is to concatenate all VCF samples by hand, removing the duplicate events:

```
zcat somefile.vcf.gz | head -n 5 > concatenated.vcf
for i in *.vcf.gz; do zcat $i | tail -n +6; done |  sort -nk 2 | uniq >> concatenated.vcf
```
The numbers "5" and "6" above relate to the number of header lines (you nay need to check one file by eye, 
or replace the `head` by a `grep` command). 
You can also [normalise the VCF files](#merging-samples-with-bcftools) before concatenating them by hand.

In a VCF file, after the special keyword headers (starting with `##`), VCF needs a file with the column description. 
If you want to rename the sample to hightlight the fact that this is a dummy sample, a concatenation of all samples,
you thus can modify this line just before the body of the file:
```
#CHROM  POS     ID      REF     ALT     QUAL    FILTER  INFO    FORMAT  all_samples
```

## Merging samples with bcftools

Tatajuba will output one VCF file per sample, but sometimes we are interested in having one file with all variants from
all samples together. 
This can be done with `bcftools merge`, which generated one multi-sample file from several individual ones. 

In the example below we assume that there are several `vcf.gz` files and the reference genome
`GCF_000148705.1_ASM14870v1_genomic.fna` in FASTA format in the same directory:

```bash
# uncompress the files since the standard zlib is not compatible with the expected BGZF format
gunzip *.vcf.gz
# normalise the files to make sure modifications are comparable
for i in *.vcf; do bcftools norm -f GCF_000148705.1_ASM14870v1_genomic.fna ${i} > norm.$i; done
# compress the files using the required BGZF format
for i in norm.*.vcf; do bgzip $i; done
# create an index for each file
for i in norm.*; do bcftools index $i; done
# merge the normalised files into one
bcftools merge -o bcf_merged.vcf norm.*.gz
```

In this example we also use `bcftools norm` to ["fix" each VCF file](https://samtools.github.io/bcftools/bcftools.html#norm)
(by left-aligning indels etc.).
